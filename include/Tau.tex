\chapter{Tau lepton reconstruction \& identification}\label{sec:tau}
% \pagestyle{plain}

The tau lepton, being the heaviest of the three discovered charged leptons in the Standard Model, plays a crucial role in understanding of matter at the most fundamental level. For example, in the context of the minimal supersymmetric extension of the SM (MSSM) \cite{Fayet:1974pd, Fayet:1977yc}, there is a special interest in searches for neutral and charged Higgs bosons decaying into a pair of tau leptons \cite{CMS:2022goy}. Furthermore, in the light of testing the Lepton Flavour Universality (LFU) several observed tensions with the SM predictions are yet to be understood \cite{HFLAV:2022pwe, Cheaib:2022ral, LHCb:2017vlu, LHCb:2021trn}. 

The precision of such analyses heavily relies on the ability in a given experiment to accurately reconstruct and separate tau leptons from background processes. However, the tau lepton stands out from the other leptons in its properties, which poses several challenges in this endeavour. In particular, it is the only lepton known to decay into hadrons, which makes it difficult to distinguish such decays (hereafter labelled as \tauh) from jets originating from QCD processes. Therefore, this Chapter will take an experimentalist's perspective and will describe, with a particular emphasis on the CMS experiment, challenges and achievements accomplished so far in the \tauh reconstruction and identification (hereafter also referred to as \enquote{tau lepton reconstruction/identification}).

The Chapter is organised as follows. In Section \ref{tau-intro} a brief overview of the tau lepton's history and its properties measured to this date is given. Section \ref{hps} introduces methods to reconstruct tau leptons in the CMS experiment, in particular a hadron-plus-strip (HPS) algorithm built on top of the Particle Flow (PF) algorithm, described in Section \ref{pf}. After the reconstruction of tau lepton candidates, an identification step has to be performed to categorise whether a given candidate originates from a genuine tau or a jet/lepton faking tau. An algorithm named DeepTau was developed for that purpose and its details -- including the recent improvement in the context of the Run 3 data taking -- will be described in Sections \ref{deeptau1} and \ref{deeptau5}. However, the algorithm has several intrinsic limitations in its design, and ongoing efforts to overcome them with new Machine Learning (ML) models will be detailed in Section \ref{tat}.

\section{Discovery \& Properties} \label{tau-intro}

The tau lepton was observed for the first time in the Mark I experiment at the SPEAR $e^+e^-$ storage ring at the Stanford Linear Accelerator Center (SLAC) in 1974 by Martin L. Perl et al. \cite{Perl:1975bf}. Fundamentally, the motivation \cite{Perl:1992ad} behind the analysis was to solve  an electron-muon problem, which manifests itself in two questions \cite{Perl:1996dk}:
\begin{itemize}
    \item Why is the muon 206.8 times heavier than electron?
    \item Why doesn't the muon decay through the process $\mu \to e + \gamma$?
\end{itemize}

One of the ideas to understand this difference was to change the perspective and search for additional heavy leptons, which, in case of their existence, could help to gain insights into the initial problem. The theoretical framework to search for such leptons was a sequential heavy lepton model due to its elegance, symmetry and simplicity \cite{Perl:past_future}. The minimalistic and main assumption it makes is the existence of pairs ($L_\alpha, \nu_\alpha$) of charged leptons and associated neutrinos with the lepton masses larger than those of the electron and the muon. 

Additionally, the sequential heavy lepton model builds upon the concept of the lepton number conservation. In the original formulation, it postulates that electron and muon each possess a unique property not possessed by other particle, a lepton family number, meaning that electron $e^-$ and its associated neutrino $\nu_e$ are assigned a lepton number $n_e = +1$, $\mu^-$ and $\nu_\mu$ receive a number $n_\mu = +1$ and antiparticles have the corresponding number negative. This lepton number should be preserved in reactions separately for each of the lepton family.

Assuming the lepton family conservation, the sequential heavy lepton model expands this principle to other lepton families with higher masses.  From these principles it follows that given a high enough mass of a heavy charged lepton $L^-$, there should exist the following decays:
\begin{enumerate}[label=D\arabic*]
    \item \label{Ltoe} $L^- \to e^- \bar{\nu_e} \nu_L $, 
    \item \label{Ltomu} $L^- \to \mu^- \bar{\nu_\mu} \nu_L $,
    \item $L^- \to \pi^- \nu_L $,
    \item $L^- \to \pi^-\pi^+\pi^- \nu_L $,
\end{enumerate}
where the former two are exactly analogous to the corresponding decay of the muon into electron and two neutrinos via the weak interaction.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Tau/tau_discovery.png}
    \caption{The observed background-subtracted cross section versus center-of-mass energy in the Mark I experiment within the detector acceptance for events with the $e^\pm\mu^\mp$ signature. \cite{Perl:1975bf}}
    \label{fig:tau-discovery}
\end{figure}

After making an additional ansatz that heavy charged leptons can be produced similarly to electrons and muons in reactions $e^+e^- \to L^+L^-$ ,  Martin L. Perl et al. proposed an elegant idea to search for them in this production mode by looking into a process where one $L$ would decay via \ref{Ltoe} and the other via \ref{Ltomu}. Such an unusual final state consisting of $e^\pm$ and $\mu^\mp$ of opposite charge and a missing energy due to neutrinos escaping detection would hint to anomalous processes appearing in the detector. An excess of such events over the small background expectations (Fig. \ref{fig:tau-discovery}) for which the analysts had \enquote{no conventional explanation} was exactly what was observed at SPEAR. After the follow-up studies \cite{Feldman:1976fm, PLUTO:1977ctk, Barbaro-Galtieri:1977kfn, Bartel:1978ii, Bacino:1978gb, Bacino:1978wj} it was finally concluded, that the observed excess indeed can be attributed to a lepton with the mass 3600 times the electron mass and 17 times the muon mass, later called \textit{tau} (from greek $\tau\rho\iota\tau o\nu$, \enquote{third}). These results together with an observation of the tau neutrino by the DONUT collaboration \cite{DONUT:2000fbd} therefore established the existence of the third generation of leptons.

Since the era of its discovery, the properties of the tau lepton has been extensively studied in several experiments including Belle, BaBar, BESIII, CLEO, KEDR, LEP experiments, PLUTO, and others. They can be summarised as follows \cite{ParticleDataGroup:2020ssz}:
\begin{itemize}
    \item Mass $m_\tau = 1776.86 \pm 0.12$ MeV.
    \item Mean lifetime $\tau = (290.3 \pm 0.5) \cdot 10^{-15}$ s, with the lifetime difference between $\tau^+$ and $\tau^-$: $(\tau_{\tau^+} - \tau_{\tau^-})/\tau_\text{average} < 7.0 \times 10^{-3}$ at 90\% C.L.  
    \item Decay modes (DM): notable feature is the existence of hadronic decays, not present for the other leptons. A brief summary of those decay modes relevant to this study is presented in Table \ref{tab:tau_decays}, inspired by \cite{CMS:2022prd}.
\end{itemize}

\begin{table}[ht!]
	\caption{Decay modes of the tau lepton with the corresponding branching fractions $\mathcal{B}$ \cite{ParticleDataGroup:2020ssz}. If applicable, intermediate known resonances contributing to decay modes are mentioned. $\text{h}^\pm$ denotes a charged hadron and the same numbers apply for the charge-conjugated decays.}
    \centering
	\begin{tabular}{c|c|c}
		Decay mode & Resonance & $\mathcal{B}$ (\%)\\
		\hline
		Leptonic decays & & \multicolumn{1}{l}{35.2}\\
        $\tau^- \to e^- \bar{\nu_e} \nu_\tau$ &  & 17.8\\
        $\tau^- \to \mu^- \bar{\nu_\mu} \nu_\tau$ &  & 17.4 \\
        \hline
        Hadronic decays  & & \multicolumn{1}{l}{64.8} \\
        $\tau^- \to \text{h}^- \nu_\tau$ & & 11.5 \\
        $\tau^- \to \text{h}^- \pi^0 \nu_\tau$ & $\rho(770)$ & 25.9 \\ 
        $\tau^- \to \text{h}^- \pi^0 \pi^0 \nu_\tau$ & $\text{a}_1(1260)$ & 9.5 \\
        $\tau^- \to \text{h}^- \text{h}^+ \text{h}^- \nu_\tau$ & $\text{a}_1(1260)$ & 9.8 \\
        $\tau^- \to \text{h}^- \text{h}^+ \text{h}^- \pi^0 \nu_\tau$ & & 4.8 \\
        Other & & 3.3 \\
	\end{tabular} \label{tab:tau_decays}
\end{table}

% \begin{table}[h]
% 	\caption{...}
%     \centering
% 	\begin{tabular}{c|c|c}
% 		Decay mode & Resonance & \mathcal{B} (\%)\\
% 		\hline
% 		Leptonic decays & & 35.2\\
%         $\tau^- \to e^- \bar{\nu_e} \nu_\tau$ &  & $17.82 \pm 0.04$\\
%         $\tau^- \to \mu^- \bar{\nu_\mu} \nu_\tau$ &  & $17.39 \pm 0.04$ \\
%         \hline
%         Hadronic decays  & & 64.8 \\
%         $\tau^- \to \text{h}^- \nu_\tau$ & & $11.51 \pm 0.05$ \\
%         $\tau^- \to \text{h}^- \pi^0 \nu_\tau$ & $\rho(770)$ & $25.93 \pm 0.09$ \\ 
%         $\tau^- \to \text{h}^- \pi^0 \pi^0 \nu_\tau$ & $\text{a}_1(1260)$ & $9.48 \pm 0.10$ \\
%         $\tau^- \to \text{h}^- \text{h}^+ \text{h}^- \nu_\tau$ & $\text{a}_1(1260)$ & $9.80 \pm 0.05$ \\
%         $\tau^- \to \text{h}^- \text{h}^+ \text{h}^- \pi^0 \nu_\tau$ & & $4.76 \pm 0.05$ \\
%         Other & & 3.3 \\
% 	\end{tabular} \label{tab:tau_decays}
% \end{table}

\section{Reconstruction in CMS}
\subsection{Particle Flow algorithm} \label{pf}
% \subsubsection{Motivation}
In order to perform physical measurements in the particle physics context, one usually operates with an abstract notion of a \textit{physics object}, which is an entity reconstructed from the signals observed in the detector and representing a particle candidate of a particular kind. The goal of the reconstruction process is to build physics objects which are as close and as representative as possible of the genuine particles appearing in the detector. Since the precision of the object reconstruction has a direct impact on the precision of the physical measurement, it therefore plays a crucial role in every particle physics analysis.

Historically, the reconstruction of particles of a given type was primarily based on the information of detector's subsystems which were specifically built to identify them. For example, the reconstruction of electrons and photons was primarily based on the ECAL response and was aimed to capture rather isolated particles. This approach can be referred to as \textit{local}, because it does not make a full use of the signals across all detector subsystems due to a technical granularity limitation. 

With detectors becoming more fine-grained one could turn from a local to a \textit{global} approach of the physics objects' reconstruction. With that it became possible to build a \textit{holistic} image of an event in the detector by linking information from various detector subsystems. This is exactly the core idea behind a \textit{particle-flow (PF) algorithm} \cite{CMS:2017yfk}, developed in the CMS experiment, which aims at tracing the entire \enquote{flow} of particles as they are traversing the detector.

\subsubsection{Basic elements}\label{sec:pf_base}
The PF algorithm follows a hierarchical approach in the reconstruction of physics objects. The first step in the algorithm is to construct basic PF elements which will later serve as a basis for building more complex high-level objects. The main PF elements being constructed at this point are:

\begin{itemize}
    \item Charged-particle tracks,
    \item Electron and muon tracks,
    \item Preshower, ECAL and HCAL energy clusters.
\end{itemize}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.95\textwidth]{Figures/Tau/track_eff.png}
    \caption{Efficiency (left) and misreconstruction rate (right) as a function of the reconstructed track \pt for the charged hadrons in a sample of simulated QCD multijet events for the track reconstruction algorithm in CMS \cite{CMS:2017yfk}. Black squares correspond to the global combinatorial track finder \cite{CMS:2006myw}, green triangles to the prompt iterations of the iterative procedure \cite{CMS:2014pgm} seeded by at least one hit in the pixel detector, red dots to all the iterations of the procedure.}
    \label{fig:track_eff}
\end{figure}

For the reconstruction of \textbf{charged-particle tracks}, a pattern recognition approach using a combinatorial Kalman filter has been an indispensable tool among experimentalists for decades, also within the CMS experiment \cite{Adam:2005cg}. To improve the overall track reconstruction efficiency while keeping the misreconstructed rate at the same level, an iterative approach is taken \cite{CMS:2014pgm}. With each fitting iteration, it targets to recover inefficiencies for a specific track type by e.g. tailoring the seed construction to a given track type, which allows the loosening of the requirement on the number of hits. The track types include prompt or displaced high/low \pt tracks, tracks inside high \pt jets, and muon tracks. Overall, the iterative procedure brings a significant recovery in efficiency across the \pt range while also performing twice faster compared to a single iteration approach due to the optimised seed construction (Fig. \ref{fig:track_eff}).

% Conceptually, it is a recursive algorithm predicting the evolution of some system in time, which at each iteration consists of two steps:
% Firstly, given an estimate of a system's state \hat{x_{t-1|t-1}} and its covariance P_{t-1|t-1} from the previous time step, a prediction of the state \hat{x_{t|t-1}} and its covariance P_{t|t-1} at the next time step is derived from a physical model describing the dynamics of the system.
% Secondly, a measurement is obtained and the new state estimate \hat{x_{t|t}} and its covariance P_{t|t} are derived as a weighted average of the already predicted and just observed information, where more certain configurations of the system receive higher weights. 

% Projecting this approach to a tracking domain, the time steps correspond to tracker layers starting from the innermost and going outwards with the dedicated generation of seeds for the first step [ref]; observations are the hits in these layers; and the physical model corresponds to the equations of motion of a charged particle in a constant magnetic field, accounting for multiple scattering and energy loss. Additionally, after the convergence of trajectory building procedure with the Kalman filter the final track fitting and smoothing is performed to determine the particle properties.

% However, the traditional pattern recognition approach had the limitation in the efficiency of charged hadron reconstruction. For example, due to the stringent requirements in the number of tracker hits which may not be satisfied for low pt particles because they undergo nuclear interactions with the detector material.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.6\textwidth]{Figures/Tau/ele_eff.png}
    \caption{Efficiency for electrons (triangles) from simulated b quark jets and charged hadrons (circles) to give rise to an electron seed as a function of \pt. Efficiencies for both ECAL-only seeding (hollow symbols) and ECAL-only with added tracker-based seeding (solid symbols) are displayed.}
    \label{fig:ele_eff}
\end{figure}

Reconstruction of \textbf{electron tracks} largely profits from the iterative tracking procedure, since the latter allows to efficiently reconstruct electrons radiating bremsstrahlung photons. It therefore forms the basis of a tracker-based seeding method, in contrast to a conventional ECAL-based, which suffers from misreconstruction of non-isolated electrons as well as radiating electrons. Generally, for electrons with a small fraction of radiated energy, the tracks are usually well-reconstructed and therefore can be extrapolated to the ECAL surface and matched with the closest ECAL cluster to form an electron candidate seed. However, when energetic photons are radiated, the pattern recognition may result in tracks fitted with large $\chi^2$ values. For such tracks, an additional fitting is performed with a Gaussian-sum filter (GSF) \cite{adam2005reconstruction}, which essentially is an adaptation of the Kalman filter fitting accounting for possible sudden and substantial energy losses along the electron's trajectory. Finally, both tracker-based and ECAL-based electron seed are passed to an extended GSF algorithm for a full track reconstruction. Overall, this procedure significantly improves electron reconstruction efficiency by up to a factor of two compared to an ECAL-based only approach, while also extending the allowed \pt range down to 2 GeV (Fig. \ref{fig:ele_eff}).

While electron track reconstruction was largely improved in the context of the PF algorithm development, \textbf{muon tracking} is not its specific part but a standalone step \cite{CMS:2018rym}. The output of this procedure is a collection of muon track candidates of three types:

\begin{itemize}
    \item \textit{Standalone-muon tracks} built by running the pattern recognition on the hits exclusively in the muon spectrometer subsystems (DT, CSC, RPC).
    \item \textit{Tracker muon tracks} reconstructed with an "inside-out" approach, where firstly the tracker tracks are extrapolated to the muon system. If at least one muon segment matches to extrapolated track, the inner tracks is declared as a tracker muon.
    \item \textit{Global muon tracks} reconstructed with an "outside-in" approach, where standalone muon tracks are being matched with tracker tracks. The matching is performed by propagating both tracks to a common surface and is followed by a combined fit with the Kalman filter. 
\end{itemize}
The resulting approach yields an excellent reconstruction of muon with about 99\% of the muons produced in the acceptance region of the detector being classified as either global or tracker muon tracks.

\textbf{Calorimeter clusters} reconstruction is an essential part of the PF algorithm. On the one hand, it plays a crucial role in the identification of neutral particles for which no track information is available, and their separation from the charged particles. On the other hand, it brings additional information to the reconstruction of electrons radiating bremsstrahlung  photons and also charged hadrons. Once combined with the track information, it helps to sizeably increase the purity of the final collection of physics objects.

The building of calorimeter clusters, performed separately in each subdetector (ECAL barrel/endcap, HCAL barrel/endcap, preshower layers), consists in two steps: 

\begin{enumerate}
    \item \textit{Clustering.} Firstly, seeds are formed from the calorimeter cells with the deposited energy larger of the neighbouring cells and above a given seed threshold. The neighbouring cells are either the four closest cells sharing a side with the seed candidate, or the eight closest cells sharing either a side or a corner with the seed candidate. Then, topological clusters are grown on the principle of recursively adding neighbouring cells, passing a certain set of energy requirements, to the current cell, starting from the seeds candidates.
    \item \textit{Energy attribution.} Due to a potential overlap of topological clusters, a dedicated approach was developed to allow for sharing of the cells energy across several clusters. Performed with an expectation-maximisation algorithm based on a Gaussian-mixture model, it assumes that the energy deposited in a topological cluster is a composition of as many spatially Gaussian-distributed energy clusters, as there are seeds in the topological clusters. The model parameters (location and total energy of each cluster in the Gaussian mixture) obtained after convergence are used as cluster parameters in the downstream reconstruction. 
\end{enumerate}

After the clusters are formed, it is important to calibrate the response of the calorimeters to have a correct energy scale and identification of neutral and charged particles. Performed for ECAL \cite{CMS:2013lxn} and HCAL \cite{CMS:2019hpr} by fitting a parametric function which maps true values of energy and pseudorapidity of the cluster to the calibrated ones, the calibration procedure was successfully validated on data and showed overall improved energy response compared to a raw one.

\subsubsection{Linking \& Object construction}
Having formed the fundamental PF elements, the PF algorithm proceeds to linking them between each other in order to form \textit{PF blocks}, essentially representing chains of PF elements. In order to reduce the computing time, the linking is performed as a search for the nearest neighbours in the $\eta-\phi$ plane via a k-dimensional tree \cite{10.1145/361002.361007}.

With the PF blocks being built, the PF algorithm proceeds to the identification of physics objects on the per block basis. The objects are formed in the following order: muons,  electrons together with isolated photons (converted or unconverted), hadrons (charged or neutral) and nonisolated photons (e.g. from $\pi^0$ decays). To account for possible nuclear interactions in the tracker material, secondary charged-particle tracks which are linked via a common nuclear-interaction vertex are merged into a single primary particle. Lastly, events are post-processed to resolve rare cases of anomalously large $\pt^{\text{miss}}$ coming from e.g. the misreconstruction of muons. A detailed description of requirements applied to PF elements within a block at each of the steps can be found in the original paper \cite{CMS:2017yfk}.

Once all PF blocks are emptied and physics objects of the aforementioned types are formed, one can proceed to grouping them into more complex objects. One of them is the tau lepton object, which reconstruction with a \textit{hadron-plus-strips (HPS) algorithm} is described in the next section. 

\subsection{HPS algorithm} \label{hps}

Since the tau lepton in about 65\% of all cases decays into the final state with hadrons (Section \ref{tau-intro}), it is important to efficiently identify such topologies in the detector. While the leptonic decays of the tau lepton in the CMS experiment are handled by the usual techniques for muon \cite{CMS:2018rym} and electron \cite{CMS:2020uim} reconstruction and identification, the hadronic decays pose a challenge of separating them from an overwhelming background of QCD jets. To tackle this, a hadron-plus-strips (HPS) algorithm was designed, originally for the LHC operation at $\sqrt{s} = $ 7 TeV \cite{cms2012performance} and 8 TeV \cite{CMS:2015pac}, followed by improvements for the data taking at $\sqrt{s} = $ 13 TeV \cite{CMS:2018jrd} and for the tau lepton identification with a DeepTau algorithm \cite{CMS:2022prd}. Below, the most recent overview of the HPS algorithm is provided with relevant references to the original implementation.

As it was previously mentioned, the main challenge in reconstructing hadronic tau decays is that of efficiently distinguishing them from a large amount of jets originating from quarks or gluons. However, hadronic decay products of the tau are usually more collimated compared to those of the QCD jets. In addition, \piz in the final state coming from intermediate $\rho(770)$ or $a_1(1260)$ resonances provide a unique handle to identify genuine \tauh as well as its corresponding decay modes (DM). In this work, DMs (Table \ref{tab:tau_decays}) are enumerated according to the formula: $\text{DM} = 5 \cross (N(\text{h}^\pm) - 1) + N(\pi^0)$, where $N(\text{h}^\pm)$ and $N(\pi^0)$ are the numbers of the reconstructed (or identified, depending in the context) charged prongs and strips/$\pi^0$, respectively. 

Motivated by these observations, the HPS algorithm starts from constructing so-called \textit{strips}, which serve as a proxy for \piz particles. In the detector, a \piz promptly decays into a pair of photons, which consequently, due to a sizeable amount of the tracker material, are very probable to convert to a pair of electrons, which can furthermore radiate bremsstrahlung photons, etc. In the presence of the magnetic field, the electrons trajectories are bent and therefore, on the ECAL surface in the $\eta-\phi$ plane, the clusters associated to \piz decay products have an extended \enquote{strip} shape in the $\phi$ direction.

In order to construct a strip, an iterative clustering procedure with the following steps is performed:

\begin{enumerate}
    \item In an event, hadronic jets are reconstructed by clustering PF particles using the anti-$k_T$ algorithm \cite{Cacciari:2008gp} (Sec. \ref{sec:jets}) with the distance parameter $\Delta R = 0.4$. For each jet, all PF particles in the cone of radius $\Delta R = \sqrt{\Delta \eta^2 + \Delta  \phi^2} = 0.5$ around the axis are passed as an input to the next step. 
    %%%
    \item Within a jet, a strip is seeded by a highest \pt photon or electron that is not yet included in any strip. The algorithm proceeds with a one-by-one aggregation of electrons/photons with $\pt > 0.5$ GeV within a $(\Delta \eta, \Delta  \phi)$ window in the $\eta$-$\phi$ plane of the dynamically adjusted size (originally, of the fixed size \cite{CMS:2015pac}). The size of the strip window is a parametrised function of \pt of the strip at the current iteration and the $e/\gamma$ to be included in the strip \cite{CMS:2018jrd}. The functional form is derived from simulated single tau events with a uniform \pt spectrum with the goal of capturing 95\% of possible $e/\gamma$ in \tauh decay products. In case of adding an $e/\gamma$ candidate to the strip, its position is recomputed as a \pt-weighted average of the coordinates in the $\eta-\phi$ plane of all the strip's constituents, and the strip momentum is set to a sum of the strip's constituents momenta. The procedure is terminated if there is no other $e/\gamma$ within a $(\Delta \eta, \Delta  \phi)$ window and the clustering of a new strip continues with selecting a new seed.
    %%%
    \item For each jet seed, \tauh hypotheses are formed by combining reconstructed strips with the charged PF candidates. Combinations are formed on the basis of decay modes to be targeted: \h, $\h \pi^0$, $\h \pi^0 \pi^0$, $\h \text{h}^\mp \h$ , $\h \text{h}^\mp \h \pi^0$, $\text{h}^\pm\text{h}^{\pm/\mp} (\pi^0)$, where \piz represents a reconstructed strip, \h a charged PF candidate and the last category targets $\tau^- \to \text{h}^- \text{h}^+ \text{h}^- \pi^0$ with one of the charged hadrons ($\pi^0$) escaping detection. The latter two were included into the reconstruction workflow together with the DeepTau algorithm \cite{CMS:2022prd}. In order to be assigned to a DM category, each combination is required to pass a mass window constraint to be compatible with the corresponding intermediate resonance (Table \ref{tab:tau_decays}). Originally, the mass window was statically defined but later it was updated to be dynamically dependant on the strip \pt. In the following, $\h \pi^0$, $\h \pi^0 \pi^0$ DMs are analysed together and referred to as $\h \pi^0$. 
    %%%
    \item Among the \tauh hypotheses formed at the previous step, a set of further requirements is applied. \tauh candidates should  have a charge $\pm 1$, except for the DMs with the missing charged hadron, where the \tauh charge is set to the charge of the charged hadron with the highest \pt. All reconstructed \h and strips in the combination should be located within the tau signal cone defined by the radius $R_\text{sig} = 3.0 / \pt \, \text{(GeV)}$, limited to the range 0.05-0.10, with respect to the \tauh momentum. Finally, for each seeding jet a single \tauh candidate with the highest \pt is selected.
    \end{enumerate}

\begin{figure}[t!]
    \centering
    \includegraphics[width=.6\textwidth]{Figures/Tau/hps_confusion.png}
    \caption{A fraction of \tauh candidates with a given generated decay mode to be reconstructed by the HPS algorithm in different decay modes \cite{CMS:2022prd}.}
    \label{fig:hps_confusion_matrix}
\end{figure}

Overall, more than a half of each of the most significant \tauh decay modes (\h, $\h \pi^0$, $\h \text{h}^\mp \h$) is reconstructed in the targeted DMs (Fig. \ref{fig:hps_confusion_matrix}). Although $\text{h}^\pm\text{h}^{\pm/\mp} (\pi^0)$ category helps to recover 19\% (13\%) of $\h \text{h}^\mp \h$ ($\h \text{h}^\mp \h \pi^0$) DMs, it is not considered in the main \tauh reconstruction routine due to its large charge mis-assignment probability. Despite the fact that DM reconstructed efficiencies are naturally bounded by the 90\% efficiency of the charged track reconstruction and even lower efficiency for photons coming from \piz decays, one can observe that there is still room for improvement in the reconstruction of all DMs. This is particularly true for the DMs with one charged prong \h and at least one \piz, where the HPS algorithm fails to reconstruct 25\% of these DMs, which amounts to $\approx$8\% of all possible tau decays. Therefore, this motivates future studies in the direction of improvement of the HPS algorithm.

\section{Identification in CMS}
Conceptually, the reconstruction step, starting from the PF algorithm (Section \ref{pf}) and going hierarchically to more complex algorithms, e.g. jet clustering or the HPS algorithm (Section \ref{hps}), aims at providing physics objects as inclusively as possible, i.e. maximising the efficiency of capturing original genuine particles. This approach inherently creates a collection of physics objects which is not pure in the objects of interest and is contaminated by background objects. Therefore, an additional step is needed to refine the purity of the collection. 

This step is usually referred to as \textit{identification} (ID), and its goal is to identify the types of objects appearing in the collection of reconstructed objects among the categories which are expected. This two-staged \enquote{RECO-ID} paradigm of building physics objects has been a standard in high-energy physics for years. However, with the emergence of powerful ML techniques, novel end-to-end approaches unifying two steps into a single one proved to be a promising and efficient solution to the problem of reconstructing physics objects \cite{CMS:2022wjj,Pata:2021oez}. 

In the RECO-ID paradigm, ML-based algorithms have also proved to bring significant improvement to the ID step. The historical evolution pattern of ID methods is moving from a so-called \textit{cut-based} (or rule-based) set of criteria to algorithms based on \textit{linear classifiers} or an ensemble of \textit{decision trees} and then finally to algorithms based on \textit{Deep Learning} (DL). The hadronic decays of the tau lepton also fit into this historical pattern, where the RECO step with the HPS algorithm was initially followed by a set of isolation criteria targeting predefined misidentification probabilities of \tauh against quark/gluon jets \cite{cms2012performance}. Later on, algorithms based on boosted decision trees (BDT) were introduced \cite{CMS:2015pac, CMS:2018jrd} each trained to distinguish \tauh from either jets, or electrons, or muons. Lastly, a DeepTau algorithm \cite{CMS:2022prd} combined previously separate classifiers into a single neural network, providing an excellent discrimination power between \tauh, jets, electrons and muons altogether. 

The DeepTau architecture in its original implementation (Section \ref{deeptau1}), referred to as DeepTau v2.1, showed a significant improvement in \tauh identification against jets and leptons, compared to the previous approaches. Building upon this milestone, several improvements have been made in the context of the Run 3 preparation as outlined in Section \ref{deeptau5}, with the corresponding model being referred to as DeepTau v2.5. 

\subsection{DeepTau v2.1} \label{deeptau1}
There is one important aspect, in addition to the already mentioned unification of jet, electron and muon discriminants, which motivated a switch towards more advanced techniques for \tauh identification -- the usage of low-level information. While hand-crafted high-level variables (also called \textit{features}), provided as an input to a BDT, generally encapsulate the object to be identified, they are still limited in the representation power by the domain knowledge of the one who designed them. Since jets, being an input to a given model for \tauh identification, inherently exhibit complex hadronisation patterns, it is expected that their behavior cannot be fully described in terms of only several variables.

In the field of Computer Vision (CV) it has been shown that Convolutional Neural Networks (CNN) trained on images learn notions of growing complexity, starting from simple patters at the first layers and capturing more complex abstract concepts at the deeper layers \cite{olah2017feature}. Since pixels in the image carry only low-level intensity information, one can therefore view the process of training a CNN model as an \textit{automated feature engineering}: more complex features are automatically learnt based on the low-level inputs. Moreover, the performance of ML models has been shown to increase as the model size grows \cite{726791,NIPS2012_c399862d,simonyan2014very,szegedy2015going,he2016deep,huang2017densely,tan2019efficientnet}. That hints towards a large scope of high-level features which models can learn without explicit guidance. Furthermore, it is yet to be understood if (and how) it is possible to design such automatically learnt features manually.

One of the perspectives on a particle detector is when it is viewed as a \enquote{camera} imaging collisions. That makes it natural to use an \textit{image representation} to describe the physics objects and the activity in the detector \cite{Cogan:2014oua,deOliveira:2015xxd,Kagan:2020yrm}. Despite the fact that this representation comes with certain limitations (Section \ref{tat}), it has proved to be very performative in tasks like jet tagging \cite{Kasieczka:2019dbj,ATLAS:2017dfg,CMS:2020poo}, particle reconstruction and identification \cite{KM3NeT:2020zod,Collado:2020ehf,Collado:2020fwm,Abbasi:2021ryj}, and particle shower generation \cite{Paganini:2017dwg,Khattak:2021ndw,Buhmann:2021caf}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=.75\textwidth]{Figures/Tau/deeptau_grids.pdf}
    \caption{The grid representation in $\eta$-$\phi$ plane passed as the input to the DeepTau model \cite{CMS:2022prd}. The signal cone ($R = 0.1$) and the isolation cone ($R = 0.5$) are also shown to motivate the choice of the inner and outer grids. The former enters the isolation requirement on the PF candidates within the HPS algorithm and has a higher cell granularity to better capture the hadronisation activity within the core of the \tauh candidate, especially in boosted scenarios. The latter enters in the computation of the high-level isolation variables used for the training and aims to capture the hadronisation activity at a larger scale to specifically identify quark and gluon jets.}
    \label{fig:deeptau_grid}
\end{figure}

All these aspects motivate the usage of convolution layers as the main building blocks of the DeepTau architecture and an image-like structure of the \tauh \textbf{input representation} (Fig. \ref{fig:deeptau_grid}). The latter is constructed by defining in $\eta$-$\phi$ space an inner grid with $11\times11$ cells of size $0.02\times0.02$, and an outer grid with $21\times21$ cells of size $0.05 \times 0.05$ (in the $\eta/\phi$ units of measurement). The grids overlap and are centered around the HPS-reconstructed direction of flight of the \tauh candidate (Section \ref{hps}). Seven types of particles in the vicinity of the \tauh-axis are taken as an input:

\begin{itemize}
    \item PF-reconstructed: muons, electrons, photons, charged hadrons, and neutral hadrons (Section \ref{pf}). 
    \item Standalone-reconstructed (RECO): electrons, muons.
\end{itemize}

The latter category uses dedicated standalone reconstruction algorithms which provide additional information about electrons and muons compared to those available from the PF algorithm. Each of the particles is attributed to a cell on both inner and outer grids according to its position in $\eta$-$\phi$ space, and the corresponding cell is filled with the features specific for a given particle type. Generally, the features describe the track quality, the quality of the associated PV or SV, the particle kinematics, the calorimeter and PU information. If several particles which are attributed to a grid of the given type \{inner, outer\} $\cross$ \{$e^\pm/\gamma$, $\mu^\pm$, $\text{h}^\pm/\text{h}^0$\} (described below) enter the same cell, the features of the one with the highest \pt are filled into the cell. 

In addition, high-level features are also provided as an input, as described below, to improve the discriminating power of the model. These handcrafted variables, describing the \tauh isolation, kinematic properties, associated vertex information, information about the associated strips, have been successfully used previously for \tauh identification with the MVA classifiers. Although in theory, the model should be able to learn these variables, in practise this is often not the case due to a limited number of training data. Therefore, these variables are added explicitly to augment the model with expert knowledge. 

The overall \textbf{architecture} is illustrated on Fig. \ref{fig:deeptau_v2p1_arch}. The model hyperparameters are described in detail in the original paper and below a conceptual overview of the model structure is provided. It starts from three streams, each processing its inputs independently: 
\begin{itemize}
    \item \textit{Global:} a set of fully connected layers which processes high-level features.
    \item \textit{Inner:} a set of one-dimensional (1D) followed by two-dimensional (2D) convolutional layers which processes inputs from the inner cone around the reconstructed tau direction of flight.
    \item \textit{Outer:} a set of 1D followed by 2D convolutional layers which processes inputs from the outer cone around the reconstructed tau direction of flight.
\end{itemize}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.\textwidth]{Figures/Tau/deeptau_v2p1.pdf}
    \caption{The DeepTau v2.1 architecture. Three processing streams (inner, outer, global) are illustrated as arrows, three particle subsections ($e^\pm/\gamma$, $\mu^\pm$, $\text{h}^\pm/\text{h}^0$) per inner/outer streams are illustrated as rectangular blocks in white. Additionally, the detailed information about the number of input features per particle block/stream, the hyperparameter values, the evolution of the input tensor shape at various stages within the model as well as the number of trainable parameters (TP) for the different subsections of the model is provided.}
    \label{fig:deeptau_v2p1_arch}
\end{figure}
1D section of both inner and outer streams are further split into three subsections individually processing three particle blocks: 
\begin{itemize}
    \item $e^\pm/\gamma$: to process inputs from combined PF electrons, PF photons, RECO electrons.
    \item $\mu^\pm$: to process inputs from combined PF muons and RECO muons.
    \item $\text{h}^\pm/\text{h}^0$: to process inputs from combined PF charged hadrons and PF neutral hadrons.
\end{itemize}

After being processed individually, the three particle blocks are concatenated and passed to another set of 1D convolutional layers before being passed to a 2D section. In general, the idea of using 1D convolutions is to encode input features on the per-particle level into a more compact representation compared to the dimensionality of the input space. Otherwise, the  usage of 2D convolutional layers directly on the input feature space would make the training task computationally hard to perform. Lastly, for each of the inner and outer streams, the 1D section is followed by the 2D section, where $3\cross3$ filters extract spacial correlations between cells across the grid while also downsampling the spatial dimensions from $11x11$ ($21\cross21$) cells for the inner (outer) grid to a single cell (Fig. \ref{fig:deeptau_v2p1_arch}). 

Overall, the three-stream part of the DeepTau architecture can be viewed as an \textit{encoder}, which extracts high-level features from the low-level ones while operating on a physically-motivated representation of $\eta$-$\phi$ space. From this perspective, the following processing part of the architecture can be viewed as a \textit{decoder}, which maps the learned features for a given \tauh candidate to a class probability. First, it concatenates extracted features from the inner and outer streams with the handcrafted high-level features after being processed in the global stream. Second, it processes them by a set of fully-connected layers, finally followed by a fully-connected layer with four output nodes with a softmax activation function. The latter outputs the probability $y_\alpha$ of the given \tauh candidate to belong to one of the four classes: electron, muon, genuine \tauh, quark or gluon jet. 

In order to measure the model performance and also derive working points (WPs), the final discriminators against electrons, muons and jets are defined as:
\begin{equation} \label{eq:d_alpha}
    D_\alpha(\bm{y}) = \dfrac{y_\tau}{y_\tau + y_\alpha},
\end{equation}
where $\bm{y} = (y_e, y_\mu, y_\tau, y_\text{jet})$ is the output of the softmax layer of the model.

To perform the training, a \textbf{loss function} is constructed and minimized with Nesterov-accelerated adaptive momentum estimation (NAdam) \cite{dozat2016incorporating}. The loss function consists of three terms (Appendix \ref{app:loss}):
\begin{enumerate}
    \item Binary cross-entropy term for \tauh class against all the other $(e, \mu, \text{jet})$ classes combined.
    \item Focal-loss \cite{lin2017focal} term for \tauh class against all other classes combined.
    \item Focal-loss terms separately for each of the $(e, \mu, \text{jet})$ classes, smoothed by a step function to target only \tauh candidates which are likely to be classified as \tauh.
\end{enumerate}
The composition of the loss function is designed to guide the training to have better performance in the regions which are important for most of the analyses. Namely, it aims to provide better performance in the range 50-80\% of \tauh efficiency, while on the other hand to not focus on the identification of background classes in the high-purity regime.   

The \textbf{data set} used for the training consists of events from the following simulated processes: Z+jets (NLO), W+jets, \ttbar, $Z^{'} \to \tau \tau$, $Z^{'} \to ee$, $Z^{'} \to \mu \mu$, (with $m(Z^{'})$ ranging from 1 to 5 TeV),and QCD multijet production. For testing, additional event samples from $\text{H} \to \tau\tau$ and Z+jets (LO) are used. To ensure that no additional biases are introduced, the \tauh candidates are sampled from the input samples such that the contribution of each class ($e$, $\mu$, \tauh, jet) in different (\pt, $\eta$) bins is the same. Furthermore, during the training additional weights are applied to make the distribution of classes uniform within each (\pt, $\eta$) bin. In total, around 140 million \tauh candidates are used for the training, while around 10 million are used for the validation. The model implementation and the training are done using the TensorFlow library \cite{tensorflow2015-whitepaper}. 

Overall, large gains in the \textbf{performance} are reported across various regions of the phase space with respect to the previous cut-based/tree-based \tauh identification approaches (Fig. \ref{fig:deeptau_v2p1_performance}). In summary, at a given \tauh efficiency, the DeepTau discriminator consistently reduces the misidentification probability against jets by more then a factor of 2. Against electrons, the improvement in misidentification probability ranges from by a factor of 2 for a \tauh efficiency of 70\% and goes up to a factor of 10 for \tauh efficiencies larger than 88\%. Against muons, the misidentification probability is reduced by almost a factor of 10 in the region of \tauh efficiency around 99\%. This expected performance improvement, obtained from the simulated events, was successfully validated on collision data, therefore establishing a new milestone for the tau identification task. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.32\textwidth]{Figures/Tau/deeptau_v2p1_vs_jet.pdf}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/deeptau_v2p1_vs_e.pdf}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/deeptau_v2p1_vs_mu.pdf}
    \caption{Efficiency for jets (left), electrons (middle), and muons (right) versus efficiency for genuine \tauh to pass various tau identification discriminators as well as the corresponding $D_\alpha$ discriminators (Eq. \ref{eq:d_alpha}). \tauh are selected with the requirements $20 < \pt < 100$ GeV, $|\eta| < 2.3$, and should additionally pass the loosest WPs against other tau types, with the thresholds as defined in the original paper. For evaluation, genuine \tauh are taken from the $H \to \tau\tau$ event sample, electrons and muons are taken from the $Z \to ll$ event sample, and jets are taken from the \ttbar event sample.}
    \label{fig:deeptau_v2p1_performance}
\end{figure}

\subsection{DeepTau v2.5} \label{deeptau5}
As the LHC and the CMS detector in particular are continuously being upgraded and are moving on with collecting new data, every particle identification model deployed in production largely benefits from retraining on the data corresponding to the new detector conditions. It ensures that the quality of the model does not degrade due to the lack of robustness to the gradual changes in the underlying data (so-called \textit{data drift}) and the model performance stays at the nominal level.

This applies also to DeepTau v2.1 (Sec. \ref{deeptau1}), which is originally trained on the data samples simulated and reconstructed with the 2017 data-taking conditions. With the start of the new Run 3 period, it is expected that the model performance will become suboptimal on the newly collected data as well as on the Run2 Ultra Legacy (UL) reprocessed data. Therefore, a dedicated DeepTau retraining has been performed with the following set of updates, further described in more details: 

\begin{itemize}
    \item Data preparation:
    \begin{itemize}
        \item Updated data samples,
        \item Shuffle \& Merge procedure,
        \item Feature preprocessing.
    \end{itemize}
    \item Training:
    \begin{itemize}
        \item Parallel data loading,
        \item Hyperparameter optimisation,
        \item Adversarial training.
    \end{itemize}
\end{itemize}

\subsubsection{Data preparation}
The global tau identification task remains unchanged w.r.t. DeepTau v2.1 in terms of the input and output spaces: for a given HPS-reconstructed \tauh candidate the goal is to predict its probability to be either of (jet, $e$, $\mu$, \tauh) classes, with the class assignment criteria described below. The \tauh candidate is represented with the surrounding PF and RECO electron/muon candidates placed onto inner and outer grids in $\eta$-$\phi$ space centered around \tauh axis, with the grid parameters being the same as defined in Sec. \ref{deeptau1}. The set of input features describing particle collections is the same as it was used for the DeepTau v2.1 training.

In order to compose the training data set, \tauh candidates of the given classes are sourced from the following data samples, simulated under the detector conditions corresponding to the 2018 year:
\begin{itemize} \label{v2.5:datagroups}
    \item DY: inclusive, jet/$H_\text{T}$-binned $\Rightarrow$ jet, $e$, $\mu$, \tauh.
    \item QCD: \pt/$H_\text{T}$-binned $\Rightarrow$ jet.
    \item \ttbar: leptonic, semileptonic $\Rightarrow$ jet, $e$, $\mu$, \tauh.
    \item \ttbar: fully-hadronic $\Rightarrow$ jet, $e$, $\mu$.
    \item W+jets: jet/$H_\text{T}$-binned $\Rightarrow$ jet, $e$, $\mu$, \tauh.
    \item Higgs: $Z \text{H}\to \tau\tau$, $W^\pm \text{H}\to \tau\tau$, $\text{H}\to \tau\tau$ (vector-boson fusion), $\text{H}\text{H}\to bb\tau\tau$ (gluon-gluon fusion) $\Rightarrow$ \tauh.
    \item $Z' \to e^+e^-$: $m(Z') \in [1000, 4000]$ GeV $\Rightarrow$ $e$.  
    \item TauGun: $\pt(\tau) \in [15, 3000]$ GeV $\Rightarrow$ \tauh.
\end{itemize}

A \tauh candidate is selected and assigned to one of the classes based on the following criteria:
\begin{itemize}
    \item $e (\mu)$: matching to a prompt $e (\mu)$ or $\taue (\taum)$ at the generator level within a cone of radius $R=0.2$. The associated generated lepton should pass the requirement $\pt^{\text{vis}}(l)>8$ GeV.
    \item \tauh: matching to a hadronically decaying tau lepton at the generator level within a cone of radius $R=0.2$. The associated generated lepton should pass the requirement $\pt^{\text{vis}}(l)>15$ GeV.
    \item jet: absence of an associated generator-level lepton and matching to a generator-level jet within a cone of radius $R=0.4$. Additionally, in the selection procedure jets with the reconstructed $\pt < 80$ GeV are randomly rejected with the probability $p = 1 - \exp{-0.05\cdot(80-\pt)}$ in order to balance the contributions from low- and high-\pt ranges in the training data set.
\end{itemize}

After the initial selection of \tauh candidates from the data samples, a training data set has to be formed as a set of \tauh candidates grouped into batches, later fed into the model. In order to have a stable training procedure, it is beneficial for batches in the data set to have the following properties:
\begin{itemize}
    \item Homogeneous: every two randomly selected batches are statistically similar. 
    \item Unbiased: every batch should be sampled in an unbiased way from the predefined target distribution $p_\text{target}(x_s)$ over a set of \textit{spectrum variables} $x_s$. In the case of DeepTau v2.5, these are selected to be: $x_s = \{\pt(\tauh), \eta(\tauh), \text{class} \in \{e, \mu, \tauh, \text{jet}\}\}$. The target distribution is chosen to be a uniform histogram (referred to as a spectral histogram) in a predefined binning for the spectrum variables. 
\end{itemize}

The following procedure, referred to as \textbf{Shuffle \& Merge} (S\&M), is used to form the training data set satisfying these properties. Performed in a memory-efficient manner, it allows for a better control over the spectrum variables compared to the balancing approach used for the DeepTau v2.1 training. It follows a stochastic approach, where a data group is firstly sampled from the categorical distribution, where the categories are the groups of the data samples used for the training as listed above, and the probability of sampling from a given data group is proportional to the number of entries in the group. Secondly, a random \tauh candidate is sampled from the data group, and it is kept for the training with the probability:
\begin{equation}
    p(\tauh) \sim \dfrac{p_\text{target}(x_s(\tau_h))}{N(\text{bin},\text{group})}.
\end{equation}

Here, $x_s(\tau_h)$ are the values of the spectrum variables for the given \tauh candidate, $N(\text{bin},\text{group})$ is the total number of events in a bin of the spectral histogram corresponding to the given $x_s(\tau_h)$ value and the data group. An additional correction to the probabilities in the spectrum histograms is made to keep the ratio between the number of \tauh candidates in the last \pt bin and the other \pt bins per each class more than 0.001. 

The procedure is distributed in a parallel manner and for a given thread a random subset of \tauh candidates of each of the data group is provided. The condition to terminate the procedure for a thread is when there is no more \tauh to select from one of the data groups. Upon the completion of the S\&M procedure, a Kolmogorov-Smirnov test is performed in order to validate the compatibility of randomly selected subsets of the formed data set between each other. Overall, an acceptable level of homogeneity is observed across the spectrum variables for each of the data group. The final data set comprised of around 100M \tauh candidates, with 70\% of them used for the training and 30\% for the validation. This data set is further referred to as a \textit{S\&M data set}.  

The last step before the training is the \textbf{feature standardisation}. Its goal is to bring the values of the input features to a common domain, which is achieved by subtracting the mean and dividing by the standard deviation, followed by a clamping to a range $[-5,5]$. Categorical features and features derived from $\pt, \eta, \phi$ are normalised by clamping to a predefined range, followed by a mapping to a range $[-1,1]$. The clamping procedure also removes outlying values, which together with the feature standardisation provides stable gradient updates during the training. The mean and the standard deviation for each of the features are derived in a cumulative manner by aggregating the sums and the counts of the feature values over the input data set. The validation is performed to check that the clamping procedure does not distort the original distributions of the input features.

\subsubsection{Training}

After the training data set is formed, for each \tauh candidate an image representation is constructed as described in Sec. \ref{deeptau1}. The corresponding tensors are combined into batches which are subsequently fed into the model during the training. Since the batch shaping procedure is performed on the fly, there is a challenge of how to make the data loading procedure time-efficient during the gradient updates. This is solved by introducing a multiprocessing queue and a set of workers filling/taking batches to/from the queue, thus allowing for a concurrent loading of batches into the model as the training is on-going. A PyTorch \cite{NEURIPS2019_9015} implementation of the multiprocessing queue is used as providing a factor of 3 speed-up compared to a default implementation in Python libraries. Moreover, a better scaling of computational performance with the number of workers and the queue size is observed for the PyTorch implementation. 

The \textbf{hyperparameter optimisation} is performed in several stages in order to select the model with the most optimal performance. For the first stage (stage 0), the training of each of the trials is performed on 20\% of the S\&M data set for one epoch, which takes approximately one day on NVIDIA$^\text{TM}$ Tesla V100. For validation, another 20\% of the S\&M data set is used. The same loss function is used as for the DeepTau v2.1 training (Appendix \ref{app:loss}) and it is minimised with a NAdam optimiser with the following parameters: learning rate = $10^{-3}, \, \beta_1 = 0.9, \, \beta_2 = 0.999, \,  \epsilon=10^{-7}$. Training weights, derived from the spectral histograms of the S\&M data set, are added to the loss function in order to make the contributions from different $(p_\text{T}, \eta, \text{class})$ bins uniform. The hyperparamater values varied during the first stage of the optimisation as well as the resulting performance are summarised in Table \ref{tab:v2p5_stage0}. The trial names correspond to the following configurations of the hyperparameters (changes are with respect to the baseline):

\begin{itemize}
    \item Baseline: DeepTau v2.1 architecture (Sec. \ref{deeptau1}).
    \item (1): number of 2D filters is reduced by 1.8 from one layer to another (*) + number of nodes in the decoder's dense layers form a progression from n/2 to 32 with reduction factor 2, where n is the input dimensionality to the decoder (**). 
    \item (2): (*) + (**) + number of filters in 1D convolutions operating on the merged $e^\pm/\gamma$, $\mu^\pm$, and $\mathrm{h}^\pm/\mathrm{h}^0$ streams forms a progression: $227 \to 141 \to 128$.
    \item (3): (*) + (**) + number of filters in 1D convolutions operating on the merged $e^\pm/\gamma$, $\mu^\pm$, and $\mathrm{h}^\pm/\mathrm{h}^0$ streams forms a progression: $227 \to 141 \to 88 \to 64 \to 40 \to 32$.
    \item (4): (**) + number of 2D filters in each layer is reduced by 2 from one layer to another.
    \item (5): (**) + number of 2D filters in each layer is reduced by 1.6 from one layer to another.
    \item (6): (*) + number of nodes in the decoder's dense layers forms a progression from n/3 to 32 with the reduction factor 3. 
    \item (7): (*) + number of nodes in the decoder's dense layers forms a progression from n/1.5 to 32 with the reduction factor 1.5. 
\end{itemize} 
\begin{table}[h]
	\caption{Performance comparison for the models from the stage 0 of the DeepTau v2.5 hyperparameter optimisation. The changes to the hyperparameters in the trials are described in the text and the number of trainable parameters (TPs) is shown in the middle column. Each trial is run 5 times and the mean and standard deviation values of the loss function on the validation set are reported in the right column.}
    \centering
	\begin{tabular}{c|c|c}
		Trial & TPs $\cross 10^{3}$  & $L_\text{val}$\\
		\hline
		Baseline & 1,151 & $\mathbf{0.273 \pm 0.002}$\\
        (1) & 1,197  & $0.277 \pm 0.003$\\
        (2)  & 3,211 & $0.274 \pm 0.002$ \\
        (3)  & 653 & $0.287	\pm 0.008$ \\
        (4)  & 768 & $0.280 \pm 0.002$ \\
        (5)  & 2,570 & $0.287 \pm 0.019$ \\ 
        (6)  & 1,097 & $0.276 \pm 0.004$ \\
        (7)  & 1,383 & $0.324 \pm 0.080$ \\
	\end{tabular} \label{tab:v2p5_stage0}
\end{table}

Overall, the architecture with the hyperparameter values of the DeepTau v2.1 model gives the best performance if measured by the value of the loss function on the validation data set averaged across 5 independent trials.

While the first stage involves the variations of the network structure, the second stage (stage 1) targets the choice of the optimiser and the learning rate. Furthermore, since at this point 70\% (30\%) of the S\&M data set is used for the training (validation), the goal is to reach better convergence compared to the short training of the first stage. The best performing model from the first stage is therefore taken and the training is continued for additional two epochs with the following optimisers and learning rates values being probed:
\begin{itemize}
    \item NAdam: $10^{-3}$ \textcolor{gray}{(acbad)}, $10^{-4}$ \textcolor{gray}{(5371f)}, $10^{-5}$ \textcolor{gray}{(08f84)}, $10^{-3}$ w/ cross-entropy \textcolor{gray}{(6945b)}.
    \item Adam \cite{kingma2014adam}: $10^{-3}$ \textcolor{gray}{(38c05)}, $10^{-4}$ \textcolor{gray}{(59162)}.
\end{itemize}

where a unique hash value for the corresponding entry in the legend of Fig. \ref{fig:v2p5_HO_stage1_performance} is specified in brackets. For all of the trials, the loss function from the first stage is used in the minimisation, except for the trial NAdam ($10^{-3}$), where the loss function consists only of the weighted sum of binary cross-entropy terms for \tauh versus the other classes.

To evaluate the performance at the second stage, receiver operating characteristic (ROC) curves are derived for each of the discriminators (Eq. \ref{eq:d_alpha}) in bins of \pt, $|\eta|$ and HPS-reconstructed decay mode of the \tauh candidate. For evaluation, genuine \tauh candidates are sourced from the $\text{H} \to \tau\tau$ (gluon-gluon fusion production mode) data sample. Jets are sourced from the fraction of the semileptonic \ttbar data sample not used in the training. Electrons and muons are sourced from the fraction of the DY data sample not used in the training. The results of the evaluation are shown on Fig. \ref{fig:v2p5_HO_stage1_performance} for the region $p_\text{T}(\tauh) \in [20,100), |\eta(\tauh)| < 2.3, \text{DM}(\tauh) \in \{0,1,10,11\}$. While Adam ($10^{-3}$) performs the best in the classification against muons, it shows inferior performance compared to the other models in the classification against electrons (low tau efficiency region) and against jets. Likewise, the model used as the starting one for the second stage tuning (referred to as \enquote{default} on Fig. \ref{fig:v2p5_HO_stage1_performance}), shows the best performance against jets but performs 10-20\% worse against electrons and muons. As a trade-off, the model which corresponds to the trial NAdam ($10^{-4}$) is chosen at this stage as the one compromising the performance against all of the three classes.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figures/Tau/v2p5_HO_st1_jet.png}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/v2p5_HO_st1_e.png}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/v2p5_HO_st1_mu.png}
    \caption{Efficiency for jets (left), electrons (middle), and muons (right) versus efficiency for genuine \tauh to pass the corresponding $D_\alpha$ discriminators for each of the trials at the second stage (stage 1) of the hyperparameter optimisation. \enquote{Default} is the model used as a starting model for all of the trials. The panel at the bottom of each of the figures shows the ratio of the ROC curves evaluated for each of the trial to the ROC curve evaluated for the starting model.}
    \label{fig:v2p5_HO_stage1_performance}
\end{figure}

The last stage (stage 2) in the training pipeline introduces an \textbf{adversarial approach} to fine-tuning the model. The motivation for that is to eliminate the discrepancies between data and simulation observed in the high-score region of $D_{\text{jet}}$ for the model after the second stage of the hyperparameter optimisation (Fig. \ref{fig:v2p5_data_mc}, left). This can be viewed as a \textit{domain shift} problem, meaning that the model performance does not transfer from the source domain on which it was trained (simulated data) to the target domain on which it is eventually being applied (collision data).

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/Tau/v2p5_data_mc_stage2.png}
    \includegraphics[width=0.48\textwidth]{Figures/Tau/v2p5_data_mc_stage3.png}
    \caption{Comparison of data and MC simulation agreement for $D_{\text{jet}}$ variable in the adversarial region as defined in the text for DeepTau v2.5 at stage 1 (left) and DeepTau v2.5 (stage 2)}
    \label{fig:v2p5_data_mc}
\end{figure}

Approaching this problem from the perspective of domain adaption \cite{wang2018deep}, one of the techniques to align the model's performance between the source and target domains is to augment the loss function with an additional adversarial term. Initially proposed in \cite{ganin2015unsupervised, Louppe:2016ylz}, the technique showed an improved modelling of displaced jets in a search for new long-lived particles in CMS \cite{CMS:2019dqq} without significant decrease in performance.  

To achieve this goal, the DeepTau v2.5 (stage 1) model is extended to have an additional stream in parallel to the decoder (Fig. \ref{fig:deeptau_v2p5_arch}). This branch is responsible for producing predictions of whether a given \tauh candidate originates from a collider data event or from a simulated data event. Conceptually, the idea is to continue training an augmented version of the model in a way that the decoder stream still tries to classify between the four classes as good as possible, while the adversarial stream tries to predict the underlying \tauh domain as bad as possible. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{Figures/Tau/deeptau_v2p5.png}
    \caption{The DeepTau v2.5 architecture. The core encoder and decoder parts along with their hyperparameters are the same as in DeepTau v2.1. The key difference is an addition of another adversarial stream in parallel to the decoder which computes the probability of a \tauh candidate to originate from either collider or simulated data.}
    \label{fig:deeptau_v2p5_arch}
\end{figure}

This intuition is implemented by adding a binary cross-entropy over two classes (collider data vs. simulated data) $L_\text{adv}$ as an additional negative term to the classification loss function $L_\text{class}$ to penalise correct identification of the data domain:
\begin{equation}
    L_\text{tot} = k_1 \cdot L_\text{class} - k_2 \cdot L_\text{adv},
\end{equation}\label{eq:adv_loss}

where $k_1, k_2 > 0$ are the hyperparameters to trade off between adversarial regularisation and classification performance. For the final model, $k_1=1$ and $k_2=10$ are chosen and each of the loss components receives its own Adam optimiser (for the classification term, the optimiser inherits its state from the original model). The initial learning rates are $0.001$ for the classification component and $0.01$ for the adversarial component and are decayed exponentially throughout the training. Lastly, the relative class importance constants in the classification loss (Appendix \ref{app:loss}) are modified: $[\kappa_e, \kappa_\mu, \kappa_\tau, \kappa_j] = \frac{4}{10}[1, 2.5, 5, 1.5] \to \frac{4}{14}[2, 5, 6, 1.]$.  

In order to perform fine-tuning with the adversarial component, an adversarial data set is formed, consisting of an equal number of collider data and simulated events. The former are taken from 2018 collider data, while the latter are sourced from Drell-Yan, \ttbar, QCD, W+jets simulated samples with the same detector conditions. The events are required to pass the selection requirements of \mt channel used in this work (Sec. \ref{sec:mt}) with an additional requirement $D_\text{jet}(\text{v2.1}) > 0.9$, where the discriminator is taken from the DeepTau v2.1 model. The region corresponding to this selection is referred to as an adversarial region. The resulting data set comprises 1.9k batches of 100 \tauh candidates (50 from collider data, 50 from simulation).

The training step firstly proceeds with passing a batch from the S\&M data set, used at the previous stages of training, and computing the gradients for the classification loss $L_\text{class}$ with respect to the encoder and decoder weights. Then, a batch from the adversarial data set is passed and the gradients for the adversarial loss $L_\text{adv}$ with respect to the encoder and the adversarial stream weights are computed. Next, the weights are updated with the computed gradients for each of the model parts according to Eq. \ref{eq:adv_loss} in order: decoder $\to$ adversarial stream $\to$ encoder. 

The training proceeds until the convergence of the loss function (\ref{eq:adv_loss}) on the validation data set, with the final validation accuracy of data vs. MC classification task equal to 0.51. This indicates that the model reaches sufficient level of not being able to distinguish between the two domains. Moreover, the agreement of data with simulation improves significantly in the adversarial region for the stage 2 model, compared to the stage 1 model before adversarial fine-tuning (Fig. \ref{fig:v2p5_data_mc}, right). 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.32\textwidth]{Figures/Tau/v2p5_HO_st2_jet.png}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/v2p5_HO_st2_e.png}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/v2p5_HO_st2_mu.png}
    \caption{Efficiency for jets (left), electrons (middle), and muons (right) versus efficiency for genuine \tauh to pass the corresponding $D_\alpha$ discriminators at the third stage (stage 2) of the hyperparameter optimisation. Three models are compared: DeepTau v2.1 (red), DeepTau v2.5 at stage 1 (green), DeepTau v2.5 at stage 2 (blue). Solid lines correspond to the performance as measured on the samples with Run 2 detector conditions which are used in the training. Dashed lines correspond to the samples with Run 3 detector conditions. The panel at the bottom of each of the figures shows the ratio of the ROC curves evaluated for each of the trial to the ROC curve evaluated for the DeepTau v2.5 at stage 1 on Run 2 samples.}
    \label{fig:v2p5_HO_stage2_performance}
\end{figure}

The model performance at stage 2 is evaluated analogously to stage 1. In addition to the simulated samples with the detector conditions of the 2018 year (referred to as Run 2 performance), the model is evaluated on Run 3 samples with the detector conditions corresponding to the early Run 3 data taking period. Electrons, muons and genuine \tauh candidates are sourced from DY sample, while jets are sourced from a \ttbar (semileptonic) sample. The resulting ROC curves evaluated in the region $p_\text{T}(\tauh) \in [20,100), |\eta(\tauh)| < 2.3, \text{DM}(\tauh) \in \{0,1,10,11\}$ are shown on Fig. \ref{fig:v2p5_HO_stage2_performance}. The performance of the model at stage 1 as well as those of DeepTau v2.1 is also shown. No significant degradation in performance is observed between the model performance at stage 1 and stage 2. The agreement between data and simulation is improved by 5-15\% as can be seen from the data/simulation ratio in the two highest score bins (Fig. \ref{fig:v2p5_data_mc}). This indicates the effectiveness of the procedure and motivates its generalisation to the discriminators against electrons and muons. However, it can be seen that the performance generally does degrade if the model is applied on the early Run 3 samples on which it was not trained. This motivates a dedicated training or fine-tuning of the model on those samples to keep the performance at the nominal level. Overall, the final DeepTau v2.5 model delivers a reduced fake rate at a given \tauh efficiency by 10-50\% across the regions of interest and sets a new improved baseline for the tau identification task.

\subsection{Tau Transformer} \label{tat}
As mentioned in Sec. \ref{deeptau1}, an image representation and a traditional convolutional approach to process it come with certain limitations. Despite yielding good results across various research domains, for the specific task of the \tauh identification there is a set of design issues:
\begin{itemize}
    \item \textbf{Information is not represented compactly.} On average, the DeepTau grid is filled with zeros in 90\% (99\%) cases for the outer (inner) grids which makes the data loading procedure not memory-efficient. The only information stored in empty cell is an implicit positional one which can be passed to the model in a more efficient way.
    \item \textbf{Convolutional layers might not encode information optimally.} Since there is no explicit communication of positional/relational information to the model, one relies on the model as capable to optimally learn the relationships between particles in the spatial 2D frame. This might happen in the limit of infinite data, but in practise, other approaches to encode information can yield better results on the limited data sets.
    \item \textbf{Translational equivariance is not applicable.} The key feature of 2D convolutional layers is that they produce representations which are equivariant with respect to translational shifts. This is not applicable to the tau identification domain, where a symmetry breaking is induced with the choice of the image centering axis (HPS-reconstructed direction of flight) and furthermore with the topology of the CMS detector. Potentially, it may result in undesired behaviour with respect to small spatial perturbations of inputs. 
    \item \textbf{Scaling with the number of PU interactions is limited.} Because of the finite cell size, in case of several particles entering the same cell only the one with the highest value of \pt is kept. On the one hand, it might provide a natural regularisation and improved robustness to the increased number of PU interactions. On the other hand, it assumes that the particles with higher \pt are more important for the \tauh identification, which might not be the case and therefore a significant loss of information can take place.  
\end{itemize}

To put the image representation into the context, there are ongoing studies of various representations in the particle physics area. Historically, representing activity in the detector as an image was one the first ideas together with a classical approach of using handcrafted features dating back to \cite{DENBY1988429}. Then, partially due to the reasons described above, sparse representations were becoming more prominent. These include sets \cite{Komiske:2018cqr}, sequences \cite{deLima:2021fwm}, graphs \cite{Thais:2022iok}, and polynomials \cite{Munoz:2022gjq} with each of these representations coming up with its own way to extract information. Furthermore, physics-motivated representations in particular aiming to preserve the underlying symmetries were also proposed and studied \cite{Dreyer:2020brq, Baldi:2022okj, Bogatskiy:2022hub, Shimmin:2021pkm}.

\subsubsection{Architecture}
In this work, a \tauh candidate is represented as a set of input particles. Taking inspiration from a Natural Language Processing (NLP) domain, this representation is tightly linked with a sentence-based perspective, where a \tauh candidate, being a set of particles (referred to as constituents or tokens), is viewed as a sentence consisting of multiple words with underlying grammar rules (for the \tauh case, decay history) which are not observed directly. In order to extract information from this representation, a self-attention mechanism is used as proposed in the original paper \cite{vaswani2017attention} introducing a Transformer model. The proposed concept of attention kick-started a revolution in the ML field due its dramatic improvement in the performance across multiple domains and due to its excellent scalability with the size of the input data set \cite{phuong2022formal}. 

In HEP domain, models built around various implementations of attention also showed noticeable improvement \cite{Mikuni:2020wpr, Mikuni:2021pou}. The most recent Particle Transformer (ParT) \cite{Qu:2022mxj} model builds upon the original Transformer model and augments it with an interaction mechanism. Notably, the importance of using larger data sets for training in the jet tagging domain is additionally emphasized.

However, the unique feature of the \tauh identification task is the heterogeneity of the input space. Transformer models in the particle physics domain so far assumed that the inputs constitute only particles of one specific kind. Furthermore, no specific treatment of global variables is proposed. This is to be contrasted with the four collections used in the DeepTau v2.1 and v2.5 training to describe a \tauh candidate: global variables, PF-reconstructed particles, RECO electrons and muons. This can be viewed from a \textit{multimodality} perspective, where an input object is described by several various modalities which cannot be \textit{a priori} combined into a single one. The notion of multimodality is strictly speaking not fully applicable to the \tauh case, since RECO electrons/muons and PF constituents are both particles in their essence. However, they have various input features, which makes their treatment as of the same kind not straight-away possible. Furthermore, global variables certainly stand out from the \enquote{particle} modality. A multimodality perspective therefore provides a convenient language to describe the input \tauh representation, also for future studies where additional collections (for example, secondary vertices or tracker hits) can be included into the input representation to make it more informative.  

A Particle Embedding module is therefore introduced (Fig. \ref{fig:tat_embedding}) in order to unify the four collections together into a single representation. The idea is to bring the dimensionality of tokens of each of the modality to a common one and combine the modalities before propagating them to attention layers. It is achieved by firstly using a categorical embedding of an additionally introduced modality variable (one-dimensional, categorical variable) with $N_\text{in}=10$ input values (7 PF types: $e$, $\mu$, $\gamma$, $\text{h}^\pm$, $\text{h}^0$, HF tower identified as an hadron, HF tower identified as an EM particle; 1 RECO muon; 1 RECO electron; 1 global variables) to an output dimensionality $d_\text{cat}=2$ (two-dimensional, real valued). Performed on a per token basis (embedding matrix is shared across the modalities), the result is concatenated to the other features and is processed by embedding blocks. Each embedding block is defined separately for each of the modalities and consists of two feed-forward layers with dimensionalities (number of output nodes) $d_\text{ff}=256$ and $d_\text{model}=64$ (Fig. \ref{fig:tat_embedding}), also operating on the per token basis. Afterwards, all the embedded tokens of all the modalities are concatenated together per \tauh candidate and passed through a dropout layer \cite{JMLR:v15:srivastava14a} with $p=0.1$ to self-attention layers.

It is worth mentioning that in this approach global variables are treated as a \enquote{context} token: it is allowed to interact with the other constituents (both PF and RECO particles) on the equal basis as constituents interact with each other. This interaction is guided via attention mechanism and is learned by the model during the optimisation procedure. However, while the proposed approach of combining modalities is straight-forward in its intuition, it might not be optimally representing the underlying relationships between them \cite{xu2022multimodal}. This question of optimally encoding multiple modalities in the context of jet tagging is left for future research.   

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figures/Tau/tat_embedding.pdf}
    \caption{A Particle Embedding block. Processing streams for RECO electrons and RECO muons are illustrated together for visualisation purposes. For a single \tauh candidate each modality is represented as a tensor of the shape $[\text{N}(*), \text{N}_\text{f}(*)]$ and the change of the shape throughout the block is shown next to the arrows. Three tokens illustrating a vector of features are shown with the grey, green, brown colors corresponding to a PF constituent, a RECO electron/muon candidate and global variables, respectively. A dotted cell represents an additionally introduced modality variable. The values of the hyperparameters ($d_\text{cat}$, $d_\text{ff}$, $d_\text{model}$) are described in the text.}
    \label{fig:tat_embedding}
\end{figure}

After the particle embedding, each \tauh candidate is represented as a tensor of the shape $[\text{N}(\text{PF})+\text{N}(e)+\text{N}(\mu)+1, \text{d}_\text{model}]$. This structure can now be processed with the encoder, which consists of the $\text{N}_l=6$ self-attention layers having the same structure of two sub-layers as in the original Transformer paper. The first sub-layer consists of a multi-head attention block with $\text{N}_\text{h}=8$ heads of dimensionality $\text{d}_\text{head}=8$ and a layer normalisation. The second sub-layer consists of two feed-forward layers with the dimensionalities $\text{d}_\text{ff, 1}=256$ and $\text{d}_\text{ff, 2}=\text{d}_\text{model}=64$. Residual connections are employed after each of the sublayers, followed by the dropout layer with $p=0.1$ and the layer normalisation. After the encoding layers, the learned embeddings are globally pooled by summing the embedding values across the token axis. Lastly, the decoder part proceeeds with $\text{N}_\text{ff}=3$ feed-forward layers with the decreasing dimensionality $256 \to 128 \to 4$ followed by the Softmax layer.   

The resulting model is referred to as Tau Transformer (TaT) and its architecture is illustrated on Fig. \ref{fig:models} (top) along with the ParticleNet \cite{Qu:2019gqs} architecture (bottom). The ParticleNet is chosen for a benchmark on the \tauh identification task as being one of the most prominent models at the moment, showing an improvement on the jet tagging tasks and successfully used in several physical analyses \cite{CMS:2022psv, CMS:2022nmn}. Its implementation follows the original paper with the exception of being adapted to a multimodality nature of the task. The modalities are embedded with the same Particle Embedding block as for TaT, with the only exception of removing the global token. The reason for that is an inherent limitation of the ParticleNet model: it assumes that the inputs to the first layer are placed in some coordinate system where the distance can be computed to define a k-nearest neighbor (kNN) graph. Since this is not applicable to the global token, it is concatenated with the learned embeddings in the encoder and further processed within the decoder. The encoder in ParticleNet consists of $\text{N}_l=3$ EdgeConv layers as proposed in the original paper with a decreasing number of the nearest neighbours $k = 16 \to 12 \to 8$, the number of channels for each of the layers $C = (160, 128, 96)$ and feature aggregation via averaging. The kNN graph in the first EdgeConv layer is constructed in the $\eta-\phi$ plane around the \tauh direction of flight. After the encoder, the global pooling of embeddings is performed via summing across the constituents axis. It is followed by the decoder consisting $\text{N}_\text{ff}=6$ of Dense layers with the decreasing dimensionality $192 \to 160 \to 128 \to 96 \to 64 \to 4$ and interleaved with the dropout layers ($p=0.1$). Overall, TaT has 416k (embedding + encoder) + 50k (decoder) = 466k , while ParticleNet has 298k (embedding + encoder) + 98k (decoder) of trainable parameters. This is to be compared with the 1314k of trainable paramerers for DeepTau v2.5. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{Figures/Tau/models.pdf}
    \caption{Tau Transformer (top) and ParticleNet (bottom) architectures as used for the \tauh identification task in this work. The hyperparameters and the structure of the layers are described in the text.}
    \label{fig:models}
\end{figure}

\subsubsection{Training setup} 

For training, 10\% of the S\&M data set (Sec. \ref{deeptau5}) is used, while another 10\% of the S\&M data set is used for validation. An extended set of features compared to DeepTau v2.5 (Sec.\ref{deeptau5}) is used to describe the input collections. For the PF candidates, additional information about the number of hits and layers in the tracker system and HCAL energy deposits is included. For the RECO electrons, a set of variables is extended with those describing a shower shape in ECAL. For the global variables, information about a relative displacement of secondary vertex from a primary vertex is included if available.  Furthermore, a collection of PF types is extended by adding the towers in the forward calorimeter reconstructed as either an hadron or an electromagnetic particle. Lastly, positional information about each of the particle is encoded as two added features ($r, \theta$), representing a radial and angular position in polar coordinates on $\eta-\phi$ plane centered around the HPS reconstructed \tauh direction of flight. No selection is applied on $r$ compared to the DeepTau case where the projection onto the grid naturally introduces a corresponding requirement. It is worth mentioning that this way of encoding relative positional information between constituents is implicit and might be suboptimal in performance. Studies of encoding it explicitly, for example, by injecting it into the attention matrices \cite{chen2021demystifying}  -- which also resembles the interaction terms from the Particle Transformer paper -- could bring more insights into how Transformer-like models can profit from inductive biases of the particle physics domain.

The training is performed by minimising with an Adam optimiser (learning rate = $1e^{-4}$, $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1e^{-7}$) a categorical cross-entropy loss function without any training weights and with the early stopping after 3 epochs. \tauh candidates are grouped into batches of 128 via a so-called uniform dynamic batching scheme. Since \tauh candidates have different number of PF and RECO constituents, a batch is padded with 0 (separately for PF and RECO modalities) for the tensors to have a regular shape. This effectively corresponds to adding for each \tauh candidate in the batch artificial \enquote{0} tokens until the maximum PF/RECO sequence length in the batch. Traditionally, \tauh candidates for batching are sampled randomly from the training data set, which translates into a random sampling of particle sequences from the underlying distribution in the training data set (Fig. \ref{fig:smart_batching}, left). Since the distribution is skewed towards \tauh candidates with a larger number of constituents, it results in a high probability for a given batch to have such a \tauh candidate, while the other \tauh candidates will have on average significantly lower number of constituents. After padding, it results in a large number of artificial tokens in a batch and on average larger batches if measured over the constituent dimension (Fig. \ref{fig:smart_batching}, right, blue distribution). It in turn translates into inefficient computation of attention weights as it scales quadratically with a sequence length. 

To mitigate this inefficiency and produce more compact batches, the training data set is divided into 30 equal bins from 0 to 300 over the number of PF constituents per \tauh candidate. Then, a single batch is allowed to be formed only from \tauh candidates sampled from a single bin. The batches are further shuffled in order to avoid bias in the training procedure. This procedure significantly reduces the number of padded tokens after batching and brings the distribution of the number of constituents per batch close to the original distribution in the training data set before padding (Fig. \ref{fig:smart_batching}, right, orange distribution). Overall, a speed-up by a factor 2-3 in the training duration compared to the traditional batching is achieved without any difference in the performance. 
\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{Figures/Tau/smart_batching.png}
    \caption{The distribution of the number of PF constituents per \tauh candidate in the training data set consisting of genuine \tauh and jet objects (left), after traditional batching (right, blue) and after uniform dynamic batching, also referred to as smart batching (right, orange). For the latter two, the distribution corresponds to the number of constituents per batch after padding.}
    \label{fig:smart_batching}
\end{figure}

\subsubsection{Experiments}

First, an impact of various modalities on the model performance is studied. Starting from the basic representation of a \tauh candidate as only a set of PF candidates, RECO electrons/muons and then global variables are added to the input representation with the model being trained for each of the three scenarios. To separate the impact of the multiclass setting, the training is performed separately for each of the three binary classification problems \tauh vs. $e/\mu/\text{jet}$. The model, with the parameters as described above, remains fixed in all of the experiments, as well as the training data set. In this particular study, the same set of input features for each of the modalities as for the DeepTau v2.5 training is used. An experiment corresponding to the model trained in a multiclass setup is additionally performed. The performance is evaluated with a ROC curve as described in Sec. \ref{deeptau5}, using the 2018 samples as in the DeepTau v2.5 case with genuine \tauh being sourced from the ggH sample, electrons and muons from the DY sample, and jets from the \ttbar (semileptonic) sample. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figures/Tau/scaling_up_png/vs_jet_ggH_TT_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \includegraphics[width=0.31\textwidth]{Figures/Tau/scaling_up_png/vs_e_ggH_DY_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/scaling_up_png/vs_mu_ggH_DY_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \caption{Efficiency for jets (left), electrons (middle), and muons (right) versus efficiency for genuine \tauh to pass the corresponding $D_\alpha$ discriminators for DeepTau v2.1, DeepTau v2.5 and TaT models. For the latter, several configurations are trained with various modalities used as an input: PF candidates only on a binary classification task (dashed red, pale), PF candidates with RECO electrons/muons (PAT e/mu in the legend) on a binary classification task (dashed red, dark), PF candidates with RECO electrons/muons and global features on a binary classification task (solid red, pale), PF candidates with RECO electrons/muons and global features on a multiclass classification task (solid red, dark). Working points (grey dots) for DeepTau v2.1 are also shown, as derived in the original paper. The panel at the bottom of each figure shows the ratio of each of the ROC curves with respect to the one of the DeepTau v2.5 model.}
    \label{fig:tat_modalities}
\end{figure}

Overall, significant gain in performance is observed from addition of both RECO electrons/muons and global features (Fig. \ref{fig:tat_modalities}, Appendix \ref{app:tat-add}). While addition of RECO electron/muons does not affect the performance against the jet scenario, it does improve it for the \tauh against the electron (in a high \tauh efficiency region, up to 10 times reduced fake rate) and against the muon (throughout the \tauh efficiency region of interest, up to 100 times reduced fake rate) scenarios. Addition of global variables closes the gap between TaT and DeepTau v2.5 performance for the scenario against electron and pushes the performance further by up to 30\% for the scenarios against jet and muon. Lastly, switching from a binary classification to a multiclass model also improves the performance. This indicates that the model with a given set of hyperparameters profits from extending the effective size of the training data set and learning a joint representation to simultaneously discriminate between the four classes.

Second, a ParticleNet model with the parameters as described above (referred to as ParticleNet v0.1) is benchmarked against the TaT model corresponding to the multiclass scenario with all the modalities (referred to as TaT v0.2). It should be mentioned that both TaT and ParticleNet models do not have an adversarial fine-tuning step, which makes the comparison with DeepTau v2.5 more optimistic. However, the performance degradation due to this is not expeted to be large, as it was shown previosly during the DeepTau v2.5 (stage 2) training. An additional requirement on the z component of an impact parameter vector of \tauh w.r.t. a primary vertex $|d_z| < 0.2$ cm is applied during evaluation to be aligned with the recommended \tauh candidate selection (Sec. \ref{sec:reco_tau}). 

The corresponding ROC curves, evaluated with the same conditions as in the modality study, are shown on Fig. \ref{fig:benchmark} and in Appendix \ref{app:tat-bench}. Overall, the TaT architecture improves upon the current baseline of DeepTau v2.5. In the most populated region $p_\text{T}(\tauh) \in [20,100), |\eta(\tauh)| < 2.3, \text{DM}(\tauh) \in \{0,1,10,11\}$, TaT consistently reduces the misidentification rate against jet by up to 30\% across the \tauh efficiency range. For the electrons and muons, the TaT performance is slightly better compared to DeepTau v2.5 by up to 10\% in the misidentification rate at the fixed \tauh efficiency. However, the performance gain of TaT in the scenarios against electron/muon is more pronounced for the other regions of the phase space, in particular for \tauh candidates reconstructed in HPS decay modes 10 and 11, where it reaches up to 50\% and 70\% reduced misidentification rate against electrons and muons, respectively (Appendix \ref{app:tat-bench}). Against jets, the ParticleNet model reaches the similar performance as TaT (DeepTau v2.5) in the low (high) \tauh efficiency region. While overall there is little difference in the performance against muons for all of the benchmarked models in the low-\pt barrel region, for the scenario against electron ParticleNet shows a performance lower than both DeepTau v2.1 and DeepTau v2.5 by a significant margin.

In general, both models can profit from further hyperparameter tuning and an increase of the training data set size. On the Transformer side, as it is pointed out in \cite{hoffmann2022training} and also hinted in the Particle Transformer paper, attention-based models scale extremely well with the increase of training data set size. Therefore, future studies on the extended data set are needed to gauge the scalability of such models on the \tauh identification task.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figures/Tau/benchmark_0_png/vs_jet_ggH_TT_dz_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/benchmark_0_png/vs_e_ggH_DY_dz_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \includegraphics[width=0.335\textwidth]{Figures/Tau/benchmark_0_png/vs_mu_ggH_DY_dz_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \caption{Efficiency for jets (left), electrons (middle), and muons (right) versus efficiency for genuine \tauh to pass the corresponding $D_\alpha$ discriminators for DeepTau v2.1 (grey), DeepTau v2.5 (black), ParticleNet v0.1 (dark cyan) and TaT v0.2 (red) models. Working points (grey dots) for DeepTau v2.1 are also shown, as derived in the original paper. The panel at the bottom of each figure shows the ratio of each of the ROC curves with respect to the one of the DeepTau v2.5 model.}
    \label{fig:benchmark}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.32\textwidth]{Figures/Tau/R_studies_png/vs_jet_ggH_TT_dz_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \includegraphics[width=0.32\textwidth]{Figures/Tau/R_studies_png/vs_e_ggH_DY_dz_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \includegraphics[width=0.327\textwidth]{Figures/Tau/R_studies_png/vs_mu_ggH_DY_dz_pt_20_100_eta_0_2.3_dm_0_1_2_10_11.png}
    \caption{Efficiency for jets (left), electrons (middle), and muons (right) versus efficiency for genuine \tauh to pass the corresponding $D_\alpha$ discriminators for a TaT architecture with the various requirements on the cone distance between the directions of flight of constituents and \tauh candidate (R). The panel at the bottom of each figure shows the ratio of each of the ROC curves with respect to the model without any requirement on the cone distance (inclusive).}
    \label{fig:r_benchmark}
\end{figure}
Last, an impact of the cone requirement on the constituents is studied for the TaT v0.2 architecture. In the original data set, used for the studies above, no selection is applied on $r$, corresponding to the radial (cone) distance in $\eta-\phi$ plane between the constituent (PF candidate or RECO electron/muon) and the HPS-reconstructed \tauh directions of flight. Additional trainings of the same architecture with the requirements on the constituents $r<0.5$, $r<0.3$ and $r<0.1$ are performed. Since training instabilities were observed during the studies with the TaT v0.2 setup, the optimiser for the training was tuned. RAdam \cite{liu2019variance} optimiser with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=1e^{-7}$ is used. Initial learning rate is set to $1e^{-4}$ and is reduced during the training by 10 after every 10 epochs.  Also, the GeLU activation function \cite{hendrycks2016gaussian} is introduced in all TaT layers instead of the previously used ReLU function.

The comparison of evaluated ROC curves for each of the trials is presented on Fig. \ref{fig:r_benchmark} and in Appendix \ref{app:tat-cone}. As expected, the performance degrades for the tightest $r < 0.1$ case, which approximately corresponds to using only the candidates from the inner grid of DeepTau (Sec. \ref{deeptau1}). The performance as measured by a misidentification rate against electrons and muons at a given \tauh efficiency improves by about 10-20\% similarly for $r < 0.3$ and $r < 0.5$ cases. However, the $r < 0.3$ scenario is not favoured in the against jet task as being too narrow to capture the hadronisation patterns of QCD jets. Overall, the study indicates that the cone distance $r < 0.5$, also corresponding to the outer grid size of DeepTau, is the optimal option.  

In general, the studies described in this work illustrate that the field of jet tagging and representation learning in HEP can largely profit from adaptation of attention-based architectures. On the side of performance, scalability, flexibility and multimodality treatment they offer a powerful alternative to graph-based architectures for analysts to model various physics objects in the detector. This motivates future studies to understand the scope of these models' performance and the broadness of their applicability.