\chapter{Introduction}\label{sec:intro}

One of the most fundamental notions in the description of nature is the one of matter. It can be thought of as a substance filling the space and transforming in time. One can wonder why the objects constituting matter are very heterogeneous in properties as defined by one's perception? It turned out to be useful to spatially zoom in into the structure of matter and define the most fundamental objects serving as building blocks. These were called particles and it was the difference in their types and interaction which explained the diversity of matter representations and related phenomena.    

However, despite the particle zoo has been continuously expanding, it turned out that there are matter representations which cannot be described as being made of already known particles. An example would be dark matter which is supposed to fill the universe in order to explain various astrophysical observation, e.g. galaxy rotation curves. Furthermore, as antimatter was discovered, one would imagine that there must be an equal amount of matter and antimatter in the universe at any given point in time  -- a statement which is rooted in another important notion of symmetry as the ruling principle of nature. However, the matter which is observable from the Earth now does not have sizeable fractions of antimatter. This raises the question whether this asymmetry was generated at some point in time, and if yes, then in which way.

Assuming that there was indeed a phase in the evolution of the universe when the matter-antimatter asymmetry was generated (so-called baryogenesis), Andrei Sakharov proposed three conditions which necessary have to be satisfied for the particle interactions to create the asymmetry \cite{Sakharov:1991}:
\begin{itemize}
    \item Baryon number violation,
    \item C-symmetry and CP-symmetry violation,
    \item Absence of thermal equilibrium.
\end{itemize}

The second item in the list corresponds to the violation of the charge (C) and combined charge-parity (CP) symmetries. The charge (parity) symmetry states that a given process occurs in the same way if the particle charges (spatial coordinates) are flipped. 

Therefore, in order to establish whether these conditions hold true in nature, one should search for potential sources of the CP violation (CPV). This search has to be necessarily performed within some theoretical framework which describes the processes occurring at the most fundamental level of matter. This framework has been built over decades and it is called the Standard Model (SM). Chapter \ref{sec:sm} provides a brief overview of its essential fundamentals required to pursue the dedicated search for CPV. 

One important property which essentially defines matter is mass. It was known experimentally that the discovered particles have it, but before 1960s it was not clear how to introduce it theoretically into the SM framework. The Brout-Englert-Higgs-Hagen-Guralnik-Kibble (BEH) mechanism was one of the proposed solutions, as described in Sec. \ref{sec:break}. However, it took almost 50 years to verify this solution experimentally by observing a particle similar in properties to the Higgs boson -- a scalar particle of the Higgs field playing a key role in the proposed mechanism. To date, multiple Higgs boson properties are measured with a good level of precision, as summarised in Sec. \ref{sec:higgs}, to give a high confidence that the observed particle is indeed the one predicted by the BEH mechanism. The CP properties are also the important ones to be established as they can provide hints to the sources of CPV in the theory and the matter-antimatter asymmetry problem of the Universe. This is the main motivation behind this work, which is dedicated to the investigation of the CP structure of the interaction between the Higgs boson and tau leptons.  

However, to pursue this study one firstly needs to be able to produce Higgs bosons in the laboratory settings. For that purpose, one builds accelerators where particles are brought to the speed close to the speed of light and then collided as the way to make them interact. Chapter \ref{sec:cms-exp} introduces the Large Hadron Collider as the worldâ€™s largest particle accelerator (Sec. \ref{sec:lhc}) which can currently be viewed as the only \enquote{Higgs factory}. The Compact Muon Solenoid (CMS) detector (Sec. \ref{sec:cms}) is nowadays one of the main instruments to get insights into the interaction of the Higgs boson with the other particles. Surrounding the interaction point, it aims to fully capture and observe emerging particles, thus allowing for the full reconstruction of the Higgs boson decay.

As it was mentioned, interaction of the Higgs boson with the tau leptons is the primary focus of this work. The interaction is probed in the Higgs decay, which therefore requires precise reconstruction and identification of tau leptons. The corresponding methods developed and applied in the CMS collaboration are described in Chapter \ref{sec:tau}. Firstly, the most fundamental Particle Flow (PF) algorithm (Sec. \ref{pf}) is used to reconstruct basic particles. The hadron-plus-strips (HPS) algorithm (Sec. \ref{hps}) is then used specifically for the tau lepton reconstruction from the PF-derived particles. An identification step usually follows the reconstruction, and in CMS it is performed using the DeepTau algorithm. In this work, version 2.1 (v2.1) of this model was used (Sec. \ref{deeptau1}), but several improvements were introduced, referred to as version 2.5 (v2.5), in order to prepare the model for the new Run 3 data-taking period (Sec. \ref{deeptau5}). Furthermore, the DeepTau architecture is known to have several intrinsic limitations. New approaches based on the graph- and attention-based models were investigated to further improve the performance of the tau identification step (Sec. \ref{tat}).

It is Chapter \ref{sec:cp-etau} which describes the analysis of CP properties of the Higgs interaction with the tau leptons, as motivated above. Since there is a multiplicity of options for the tau lepton to decay, this work focuses on the reconstruction of the Higgs boson decaying into a pair of tau leptons, where one lepton decays into the final state with a single electron and neutrinos, while the other lepton decays into hadrons and neutrino (\et final state). The framework to quantify the CP effects of interest, the physical observable, and the analysis strategy are outlined in Sec. \ref{sec:cp-intro} and Sec. \ref{sec:sec:phicp}, while the other sections cover the other essential steps of the analysis. The measurement performed in the $\et$ final state (Sec. \ref{sec:results}) is combined with the measurement of the \mt and \tata final states, as described in Chapter \ref{sec:comb}. Finally, the summary and conclusions are given in Chapter \ref{sec:outro}.

\subsection*{Personal contribution}
The author of this work contributed to all the stages of the analysis of the \et final state. This includes addition of the electron objects to the analysis framework with the necessary corrections, validation of the simulation agreement with the data, training of the neural networks used for event categorisation, addition of systematic uncertainties to the analysis framework and their validation, performing statistical inference, including goodness-of-fit tests and final result extraction. 
% \begin{itemize}
%     \item Addition of the electron objects to the analysis framework with the necessary corrections,
%     \item Validation of the simulation agreement with the data,
%     \item Training of the neural networks used for event categorisation
%     \item Performing synchronisation of analysis frameworks between analysis groups,
%     \item Addition of systematic uncertainties to the analysis framework and their validation,
%     \item Performing statistical inference including goodness-of-fit tests and final result extraction.
% \end{itemize}

In the tau lepton identification studies, on the side of DeepTau v2.5, the author contributed to the improvement of the scalability of the framework, architecture optimisation, development of the feature preprocessing and performance evaluation modules, and integration into the CMS software. On the side of Tau Transformer, the contributions include the conceptual design of the model and its implementation, development of the data loading pipeline, training and hyperparameter tuning, performance evaluation, and ablation studies.