\thispagestyle{empty}
\vspace{-3cm}
\section*{\centering Zusammenfassung}
\noindent

Die Messung der CP-Eigenschaften der Yukawa-Kopplung des Higgs-Bosons an $\tau$-Leptonen wird vorgestellt. Der für die Analyse verwendete Datensatz wurde vom CMS-Experiment am LHC während der Datenerfassungsperiode des Run 2 in Proton-Proton-Kollisionen bei $\sqrt{s}=13$ TeV gesammelt und entspricht einer integrierten Luminosität von 137 \ifb. Die Yukawa-Kopplung zwischen dem Higgs-Boson und $\tau$-Leptonen wird durch den effektiven Mischungswinkel \mixa parametrisiert, wobei der Wert $\mixa = 0^\circ (90^\circ)$ dem SM-Szenario der reinen CP-even (CP-odd) $\text{H}\tau\tau$-Kopplung entspricht. 

Der Winkel zwischen den Zerfallsebenen der $\tau$-Leptonen wird als Beobachtungsgröße verwendet, die die CP-Natur des Higgs-Bosons kodiert. Die Messung wird im \et-Kanal durchgeführt, bei dem ein $\tau$-Lepton in ein einzelnes Elektron und das andere hadronisch zerfällt. Die Ergebnisse werden mit den Messungen in den Kanälen \mt und \tata kombiniert. Der beobachtete (erwartete) Wert des effektiven Mischungswinkels für die Kombination wird wie folgt gemessen:

\begin{equation}
    \mixa = -1 \pm 19^\circ (0 \pm 21^\circ) ~@68.3\% \text{ CL}.
\end{equation}

Die Ergebnisse sind mit der SM-Erwartung vereinbar, und die reine CP-odd-Hypothese wird auf dem beobachteten (erwarteten) Signifikanzniveau von $3.0 ~(2.6)$ Standardabweichungen zurückgewiesen.

Die Verbesserungen bei der Identifizierung von $\tau$-Leptonen in CMS im Zusammenhang mit der Vorbereitung von Run 3 werden beschrieben. Der DeepTau-Algorithmus wird neu trainiert und optimiert, wobei ein Verfahren zur Feinabstimmung mit dem Gegner hinzugefügt wird. Das sich daraus ergebende Modell verbessert das vorherige DeepTau-Modell in Bezug auf die Unterdrückung des Hintergrunds um 10-50\% und beschreibt die Daten mit der Simulation im \htt-Auswahlbereich besser. 

Ein neuer Algorithmus namens Tau Transformer (TaT) wird vorgeschlagen, um die Einschränkungen der DeepTau-Architektur zu überwinden. Der TaT-Kern basiert auf Self-Attention Schichten und verfügt über ein Einbettungsmodul, das die multimodale Behandlung der Eingabedarstellung ermöglicht. Der Vergleich des TaT-Modells mit dem neu trainierten DeepTau-Modell und einer vergleichbaren ParticleNet-basierten Architektur zeigt eine durchgängig verbesserte Leistung von bis zu 50\% bei der Fehlerkennungsrate in den interessierenden Bereichen \pt, $\eta$ und Decay Mode.   

% \newpage
% \thispagestyle{empty}
% \mbox{}
