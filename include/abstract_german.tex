\thispagestyle{empty}
\vspace{-3cm}
\section*{\centering Zusammenfassung}
\noindent

Die Messung der CP-Eigenschaften der Yukawa-Kopplung des Higgs-Bosons an $\tau$-Leptonen wird vorgestellt. Der f\"ur die Analyse verwendete Datensatz wurde vom CMS-Experiment am LHC w\"ahrend der Datenerfassungsperiode des Run 2 in Proton-Proton-Kollisionen bei $\sqrt{s}=13$ TeV gesammelt und entspricht einer integrierten Luminosit\"at von 137 \ifb. Die Yukawa-Kopplung zwischen dem Higgs-Boson und $\tau$-Leptonen wird durch den effektiven Mischungswinkel \mixa parametrisiert, wobei der Wert $\mixa = 0^\circ (90^\circ)$ dem SM-Szenario der reinen CP-even (CP-odd) $\text{H}\tau\tau$-Kopplung entspricht. 

Der Winkel zwischen den Zerfallsebenen der $\tau$-Leptonen wird als Beobachtungsgr\"o{\ss}e verwendet, die die CP-Natur des Higgs-Bosons kodiert. Die Messung wird im \et-Zerfallskanal durchgef\"uhrt, bei dem ein $\tau$-Lepton in ein einzelnes Elektron und das andere hadronisch zerf\"allt. Die Ergebnisse werden mit den Messungen in den Zerfallskan\"alen \mt und \tata kombiniert. Der beobachtete (erwartete) Wert des effektiven Mischungswinkels f\"ur die Kombination wird wie folgt gemessen:

\begin{equation}
    \mixa = -1 \pm 19^\circ (0 \pm 21^\circ) ~@68.3\% \text{ CL}.
\end{equation}

Die Ergebnisse sind mit der SM-Erwartung vereinbar, und die reine CP-odd-Hypothese wird auf dem beobachteten (erwarteten) Signifikanzniveau von $3.0 ~(2.6)$ Standardabweichungen zur\"uckgewiesen.

Die Verbesserungen bei der Identifizierung von $\tau$-Leptonen in CMS im Zusammenhang mit der Vorbereitung von Run 3 werden beschrieben. Der DeepTau-Algorithmus wird unter Hinzunahme des Adversarial-Verfahrens neu trainiert und optimiert. Das sich daraus ergebende Modell verbessert das vorherige DeepTau-Modell in Bezug auf die Unterdr\"uckung des Untergrundprozesses um 10-50\% und beschreibt die Daten mit der Simulation im \htt-Auswahlbereich besser. 

Ein neuer Algorithmus namens Tau Transformer (TaT) wird vorgeschlagen, um die Einschr\"ankungen der DeepTau-Architektur zu \"uberwinden. Der TaT-Kern basiert auf Self-Attention Schichten und verf\"ugt \"uber ein Einbettungsmodul, das die multimodale Behandlung der Eingabedarstellung erm\"oglicht. Der Vergleich des TaT-Modells mit dem neu trainierten DeepTau-Modell und einer vergleichbaren ParticleNet-basierten Architektur zeigt eine durchg\"angig verbesserte Leistung von bis zu 50\% bei der Fehlerkennungsrate in den interessierenden Bereichen \pt, $\eta$ und \tauh-Zerfallmodus.   

% \newpage
% \thispagestyle{empty}
% \mbox{}
