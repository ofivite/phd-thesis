\thispagestyle{empty}
\vspace{-3cm}
\section*{\centering Zusammenfassung}
\noindent

Die Messung der CP-Eigenschaften der Yukawa-Kopplung des Higgs-Bosons an $\tau$-Leptonen wird vorgestellt. Der f\"ur die Analyse verwendete Datensatz wurde vom CMS-Experiment am LHC w\"ahrend der Datenerfassungsperiode des Run 2 in Proton-Proton-Kollisionen bei $\sqrt{s}=13$ TeV aufgezeichnet und entspricht einer integrierten Luminosit\"at von 137 \ifb. Die Yukawa-Kopplung zwischen dem Higgs-Boson und $\tau$-Leptonen wird durch den effektiven Mischungswinkel \mixa parametrisiert, wobei der Wert $\mixa = 0^\circ (90^\circ)$ dem SM-Szenario der reinen CP-geraden (CP-ungeraden) $\text{H}\tau\tau$-Kopplung entspricht. 

Der Winkel zwischen den Zerfallsebenen der $\tau$-Leptonen wird als Beobachtungsgr\"o{\ss}e verwendet, deren Verteilung die CP-Natur des Higgs-Bosons widerspiegelt. Die Messung wird im \et-Zerfallskanal durchgef\"uhrt, bei dem ein $\tau$-Lepton in ein Elektron und das andere hadronisch zerf\"allt. Die Ergebnisse werden mit den Messungen in den Zerfallskan\"alen \mt und \tata kombiniert. Der beobachtete (erwartete) Wert des effektiven Mischungswinkels f\"ur die Kombination wird wie folgt gemessen:

\begin{equation}
    \mixa = -1 \pm 19^\circ (0 \pm 21^\circ) ~@68.3\% \text{ CL}.
\end{equation}

Die Ergebnisse sind mit der SM-Erwartung vereinbar, und die reine CP-ungerade-Hypothese wird auf dem beobachteten (erwarteten) Signifikanzniveau von $3.0 ~(2.6)$ Standardabweichungen zur\"uckgewiesen.

Die Verbesserungen bei der Identifizierung von $\tau$-Leptonen in CMS im Zusammenhang mit der Vorbereitung von Run 3 werden beschrieben. Der DeepTau-Algorithmus wird unter Anwendung des Adversarial-Verfahrens neu trainiert und optimiert. Das sich daraus ergebende Modell verbessert das vorherige DeepTau-Modell in Bezug auf die Unterdr\"uckung des Untergrundprozesses um 10-50\% und f\"uhrt zu einer besseren Beschreibung der Daten durch die Simulation im \htt-Selektionsbereich. 

Ein neuer Algorithmus namens Tau Transformer (TaT) wird vorgeschlagen, um die Limitierungen der DeepTau-Architektur zu \"uberwinden. Der TaT-Kern basiert auf Self-Attention Schichten und verf\"ugt \"uber ein Einbettungsmodul, das die multimodale Behandlung der Eingabedarstellung erm\"oglicht. Der Vergleich des TaT-Modells mit dem neu trainierten DeepTau-Modell und einer vergleichbaren ParticleNet-basierten Architektur zeigt eine durchg\"angig verbesserte Leistung von bis zu 50\% bei der Fehlerkennungsrate in den interessierenden \pt- und $\eta$-Bereichen sowie \tauh-Zerfallmoden.   

% \newpage
% \thispagestyle{empty}
% \mbox{}
